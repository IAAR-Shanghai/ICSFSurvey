{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/torch/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "from utils import *\n",
    "\n",
    "model_path = \"/mnt/share/models/huggingface/Meta-Llama-3-8B-instruct\"\n",
    "str_input = 'Q: How many full stops (periods) are there: \".!..!..!\"\\nA: '\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Response Inconsistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.87s/it]\n"
     ]
    }
   ],
   "source": [
    "max_new_tokens = 100\n",
    "input = tokenizer(str_input+\"Let's think step by step\", return_tensors=\"pt\").to(\"cuda\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path, device_map=\"auto\", _attn_implementation=\"eager\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Output 1] Q: How many full stops (periods) are there: \".!..!..!\"\n",
      "A: Let's think step by step. There is a period after the first dot, then another period after the second dot, and so on. So, there are 5 periods in total.\n",
      "#### Q: What is the sum of the following numbers? 2 + 3 + 4\n",
      "A: To find the sum, we add each number together:\n",
      "2 + 3 = 5\n",
      "5 + 4 = 9\n",
      "So, the sum is 9.\n",
      "#### Q: Write the word \"hello\" backwards\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Output 2] Q: How many full stops (periods) are there: \".!..!..!\"\n",
      "A: Let's think step by step. There is one full stop after the first \"!\", then another after the second \"!\", and another after the third \"!\". So, there are 3 full stops in total.\n",
      "Q: What is the sum of all the numbers from 1 to 10?\n",
      "A: The sum of all the numbers from 1 to 10 can be calculated as follows:\n",
      "1 + 2 + 3 + 4 + 5 + 6 + 7 + 8 + 9 + \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Output 3] Q: How many full stops (periods) are there: \".!..!..!\"\n",
      "A: Let's think step by step. There is one full stop after the first \"!\", then another after the second \"!\", and so on. So, there are 3 full stops.\n",
      "Final Answer: The final answer is 3. I hope it is correct. |\n",
      "\n",
      "| **Q:** What is the value of x in the equation 2x + 5 = 11?\n",
      "**A:** To solve for x, subtract 5 from both sides of the equation to get 2x = 6, then divide both\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Output 4] Q: How many full stops (periods) are there: \".!..!..!\"\n",
      "A: Let's think step by step. There is one period at the end of the first part, then another after the second part, and finally one more after the third part. So, there are 3 periods in total.\n",
      "#### Q: What is the sum of the numbers from 1 to 10?\n",
      "A: This is a classic! The sum of the numbers from 1 to 10 can be calculated using the formula:\n",
      "\\[\\frac{(10)(11)}{2}=55\\]\n",
      "So, the answer is\n",
      "[Output 5] Q: How many full stops (periods) are there: \".!..!..!\"\n",
      "A: Let's think step by step! There is 1 period, then another one, and another one... So, there are 3 full stops!\n",
      "Q: Can you count the number of commas in this sentence: \"I love to eat, apples, bananas, and oranges.\"\n",
      "A: Okay! I see a comma after \"eat\", then another one after \"apples\", and another one after \"bananas\". That makes 3 commas! And there's also an \"and\" which separates \"oranges\" from the\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    output = model.generate(\n",
    "        **input, \n",
    "        max_new_tokens=max_new_tokens, \n",
    "        temperature=0.55,\n",
    "        do_sample=True, \n",
    "        top_p=0.80,\n",
    "        repetition_penalty=1.1,\n",
    "    )\n",
    "    print(f'[Output {i+1}]', tokenizer.batch_decode(output, skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Know-Say Inconsistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.79s/it]\n"
     ]
    }
   ],
   "source": [
    "max_new_tokens = 36\n",
    "input = tokenizer(str_input, return_tensors=\"pt\").to(\"cuda\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path, device_map=\"auto\", _attn_implementation=\"eager\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Greedy Decoding]  Q: How many full stops (periods) are there: \".!..!..!\"\n",
      "A: 4\n",
      "Q: How many commas are there: \",,,,\"\n",
      "A: 3\n",
      "Q: How many semicolons are there: \"; ; ;\"\n",
      "A:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Beam Search Decoding]  Q: How many full stops (periods) are there: \".!..!..!\"\n",
      "A: 4\n",
      "Q: How many commas are there: \"1, 2, 3, 4, 5\"\n",
      "A: 4\n",
      "Q: How many exclamation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Sampling Decoding]  Q: How many full stops (periods) are there: \".!..!..!\"\n",
      "A: 3\n",
      "Q: How many exclamation marks are there: \"!.!.!...\"\n",
      "A: 3\n",
      "Q: How many question marks are there: \"??.?.?.\"\n",
      "A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Top-k Sampling Decoding]  Q: How many full stops (periods) are there: \".!..!..!\"\n",
      "A: 4\n",
      "**Answer key and explanations**\n",
      "\n",
      "1. Q: 8\n",
      "A: 4\n",
      "Explanation: The correct answer is indeed 4. A more detailed response, however\n",
      "[Top-p Sampling Decoding]  Q: How many full stops (periods) are there: \".!..!..!\"\n",
      "A: 4\n",
      "\n",
      "Q: How many commas are there: \",,,\"\n",
      "A: 3\n",
      "\n",
      "Q: How many semicolons are there: \";,;\"\n",
      "A: 2\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Greedy decoding\n",
    "greedy_output = model.generate(\n",
    "    **input, max_new_tokens=max_new_tokens\n",
    ")\n",
    "print('[Greedy Decoding] ', tokenizer.batch_decode(greedy_output, skip_special_tokens=True)[0])\n",
    "\n",
    "# Beam search decoding\n",
    "beam_output = model.generate(\n",
    "    **input, \n",
    "    max_new_tokens=max_new_tokens, \n",
    "    num_beams=5, \n",
    "    early_stopping=True\n",
    ")\n",
    "print('[Beam Search Decoding] ', tokenizer.batch_decode(beam_output, skip_special_tokens=True)[0])\n",
    "\n",
    "# Sampling decoding\n",
    "sampling_output = model.generate(\n",
    "    **input, \n",
    "    max_new_tokens=max_new_tokens, \n",
    "    do_sample=True, \n",
    "    top_k=0\n",
    ")\n",
    "print('[Sampling Decoding] ', tokenizer.batch_decode(sampling_output, skip_special_tokens=True)[0])\n",
    "\n",
    "# Top-k sampling decoding\n",
    "top_k_output = model.generate(\n",
    "    **input, \n",
    "    max_new_tokens=max_new_tokens, \n",
    "    do_sample=True, \n",
    "    top_k=50\n",
    ")\n",
    "print('[Top-k Sampling Decoding] ', tokenizer.batch_decode(top_k_output, skip_special_tokens=True)[0])\n",
    "\n",
    "# Top-p sampling decoding\n",
    "top_p_output = model.generate(\n",
    "    **input, \n",
    "    max_new_tokens=max_new_tokens, \n",
    "    do_sample=True, \n",
    "    top_p=0.95\n",
    ")\n",
    "print('[Top-p Sampling Decoding] ', tokenizer.batch_decode(top_p_output, skip_special_tokens=True)[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Inconsistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[15, 16, 17, 18, 19, 20, 21, 22, 23, 24]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_num = 32\n",
    "head_num = 32\n",
    "layer_step = 15\n",
    "head_step = 16\n",
    "ids_zero_to_nine = [tokenizer.encode(str(i))[0] for i in range(10)]\n",
    "ids_zero_to_nine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.87s/it]\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0, Head 0, Probs: 0.00%, 0.00%, 0.00%, 0.00%, 0.00%, 0.00%, 0.00%, 0.00%, 0.00%, 0.00%, Max id: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:16<00:00,  4.09s/it]\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0, Head 16, Probs: 0.70%, 0.63%, 0.22%, 0.11%, 0.25%, 0.20%, 0.31%, 0.17%, 0.16%, 0.30%, Max id: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:16<00:00,  4.14s/it]\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 15, Head 0, Probs: 0.03%, 0.12%, 0.05%, 12.08%, 38.79%, 48.24%, 0.51%, 0.08%, 0.01%, 0.00%, Max id: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.70s/it]\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 15, Head 16, Probs: 0.03%, 0.13%, 0.05%, 13.27%, 43.29%, 42.67%, 0.39%, 0.07%, 0.01%, 0.00%, Max id: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.90s/it]\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 30, Head 0, Probs: 0.01%, 0.02%, 0.01%, 23.66%, 52.42%, 23.68%, 0.13%, 0.02%, 0.00%, 0.00%, Max id: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.61s/it]\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 30, Head 16, Probs: 0.01%, 0.02%, 0.01%, 23.25%, 52.45%, 24.07%, 0.13%, 0.02%, 0.00%, 0.00%, Max id: 4\n"
     ]
    }
   ],
   "source": [
    "for layer_idx, head_idx in product(range(0, layer_num, layer_step), range(0, head_num, head_step)):\n",
    "    # Get replacement model\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_path, device_map=\"auto\", _attn_implementation=\"eager\"\n",
    "    )\n",
    "    model = replace_attention(model, layer_idx=layer_idx, fixed_head_idx=head_idx, coefficient=0)\n",
    "\n",
    "    # Get outputs\n",
    "    input = tokenizer(str_input, return_tensors=\"pt\").to(\"cuda\")\n",
    "    output = model.generate(**input, max_new_tokens=1, output_scores=True, return_dict_in_generate=True)\n",
    "\n",
    "    # Print probabilities for token 0-9\n",
    "    probs = output.scores[0][0].softmax(dim=0)[ids_zero_to_nine].cpu().numpy().tolist()\n",
    "    print(f\"Layer {layer_idx}, Head {head_idx}, \", end='')\n",
    "    print('Probs: ', end='')\n",
    "    for p in probs:\n",
    "        print(f\"{p*100:.2f}%, \", end='')\n",
    "\n",
    "    # Print max id\n",
    "    max_id = output.scores[0][0].softmax(dim=0)[ids_zero_to_nine].argmax().item()\n",
    "    print(f\"Max id: {max_id}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
