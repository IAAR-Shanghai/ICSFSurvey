@IEEEtranBSTCTL{IEEEexample:BSTcontrol,
  CTLuse_forced_etal       = "yes",
  CTLmax_names_forced_etal = "6",
  CTLnames_show_etal       = "6" 
}  % control author length


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 综述

@article{CoX_24_arXiv_SJTU,
  title={Beyond chain-of-thought: A survey of chain-of-x paradigms for llms},
  author={Xia, Yu and Wang, Rui and Liu, Xu and Li, Mingyan and Yu, Tong and Chen, Xiang and McAuley, Julian and Li, Shuai},
  journal={arXiv preprint arXiv:2404.15676},
  year={2024}
}

@article{SurveyUncertainty_23_arXiv_Nankai,
  title={Uncertainty in natural language processing: Sources, quantification, and applications},
  author={Hu, Mengting and Zhang, Zhen and Zhao, Shiwan and Huang, Minlie and Wu, Bingzhe},
  journal={arXiv preprint arXiv:2306.04459},
  year={2023}
}

@article{SelfEvolution_24_arXiv_PKU,
  title={A survey on self-evolution of large language models},
  author={Tao, Zhengwei and Lin, Ting-En and Chen, Xiancai and Li, Hangyu and Wu, Yuchuan and Li, Yongbin and Jin, Zhi and Huang, Fei and Tao, Dacheng and Zhou, Jingren},
  journal={arXiv preprint arXiv:2404.14387},
  year={2024}
}

@article{SurveyReason_23_EMNLP_Illinois,
  title={Towards reasoning in large language models: A survey},
  author={Huang, Jie and Chang, Kevin Chen-Chuan},
  journal={arXiv preprint arXiv:2212.10403},
  year={2022}
}

@article{SurveyXofThought_24_arXiv_ETH,
  title={Demystifying Chains, Trees, and Graphs of Thoughts},
  author={Besta, Maciej and Memedi, Florim and Zhang, Zhenyu and Gerstenberger, Robert and Piao, Guangyuan and Blach, Nils and Nyczyk, Piotr and Copik, Marcin and Kwasniewski, Grzegorz and M{\"u}ller, J{\"u}rgen and others},
  journal={arXiv preprint arXiv:2401.14295},
  year={2024}
}

@article{SurveyAgent_24_Frontiers_RUC,
  author       = {Lei Wang and Chen Ma and Xueyang Feng and Zeyu Zhang and Hao Yang and Jingsen Zhang and Zhiyuan Chen and Jiakai Tang and Xu Chen and Yankai Lin and Wayne Xin Zhao and Zhewei Wei and Jirong Wen},
  title        = {A survey on large language model based autonomous agents},
  journaltitle = {Frontiers of Computer Science},
  volume       = {18},
  number       = {6},
  pages        = {186345},
  date         = {2024-03-22},
  doi          = {10.1007/s11704-024-40231-1},
  url          = {https://doi.org/10.1007/s11704-024-40231-1},
  issn         = {2095-2236},
}

@article{SurveySelfCorrection_24_TACL_UCSB,
    title = "Automatically Correcting Large Language Models: Surveying the Landscape of Diverse Automated Correction Strategies",
    author = "Pan, Liangming  and
      Saxon, Michael  and
      Xu, Wenda  and
      Nathani, Deepak  and
      Wang, Xinyi  and
      Wang, William Yang",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "11",
    year = "2024",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/2024.tacl-1.27",
    doi = "10.1162/tacl_a_00660",
    pages = "484--506",
}

@article{SurveySelfCorrection_24_arXiv_PSU,
  title={When Can LLMs Actually Correct Their Own Mistakes? A Critical Survey of Self-Correction of LLMs},
  author={Kamoi, Ryo and Zhang, Yusen and Zhang, Nan and Han, Jiawei and Zhang, Rui},
  journal={arXiv preprint arXiv:2406.01297},
  year={2024}
}

@article{gao2023retrievalaugmented,
  title={Retrieval-augmented generation for large language models: A survey},
  author={Gao, Yunfan and Xiong, Yun and Gao, Xinyu and Jia, Kangxiang and Pan, Jinliu and Bi, Yuxi and Dai, Yi and Sun, Jiawei and Wang, Haofen},
  journal={arXiv preprint arXiv:2312.10997},
  year={2023}
}

@article{SurveyKD_23_arXiv_McGill,
  title={Teacher-student architecture for knowledge distillation: A survey},
  author={Hu, Chengming and Li, Xuan and Liu, Dan and Wu, Haolun and Chen, Xi and Wang, Ju and Liu, Xue},
  journal={arXiv preprint arXiv:2308.04268},
  year={2023}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 理论

@article{TheorySnowball_23_arXiv_NYU,
  title={How language model hallucinations can snowball},
  author={Zhang, Muru and Press, Ofir and Merrill, William and Liu, Alisa and Smith, Noah A},
  journal={arXiv preprint arXiv:2305.13534},
  year={2023}
}

@article{TheoryLatentReason_24_arXiv_Google,
  title={Do Large Language Models Latently Perform Multi-Hop Reasoning?},
  author={Yang, Sohee and Gribovskaya, Elena and Kassner, Nora and Geva, Mor and Riedel, Sebastian},
  journal={arXiv preprint arXiv:2402.16837},
  year={2024}
}

@article{ExpressUncertainty_24_arXiv_TAU,
  title={Can Large Language Models Faithfully Express Their Intrinsic Uncertainty in Words?},
  author={Yona, Gal and Aharoni, Roee and Geva, Mor},
  journal={arXiv preprint arXiv:2405.16908},
  year={2024}
}

@inproceedings{TheoryNoReason_24_ICLR_Google,
title={Large Language Models Cannot Self-Correct Reasoning Yet},
author={Jie Huang and Xinyun Chen and Swaroop Mishra and Huaixiu Steven Zheng and Adams Wei Yu and Xinying Song and Denny Zhou},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=IkmD3fKBPQ}
}

@article{CalibIC_24_arXiv_SJTU,
  title={Calibrating Reasoning in Language Models with Internal Consistency},
  author={Xie, Zhihui and Guo, Jizhou and Yu, Tong and Li, Shuai},
  journal={arXiv preprint arXiv:2405.18711},
  year={2024}
}

@article{SelfIncorrect_24_arXiv_JHU,
  title={Self-[in] correct: Llms struggle with refining self-generated responses},
  author={Jiang, Dongwei and Zhang, Jingyu and Weller, Orion and Weir, Nathaniel and Van Durme, Benjamin and Khashabi, Daniel},
  journal={arXiv preprint arXiv:2404.04298},
  year={2024}
}

@inproceedings{GPT4Doesnt_23_NeurIPS_ASU,
title={{GPT}-4 Doesn{\textquoteright}t Know It{\textquoteright}s Wrong: An Analysis of Iterative Prompting for Reasoning Problems},
author={Kaya Stechly and Matthew Marquez and Subbarao Kambhampati},
booktitle={NeurIPS 2023 Foundation Models for Decision Making Workshop},
year={2023},
url={https://openreview.net/forum?id=PMtZjDYB68}
}

@inproceedings{CanSelfCritique_23_NeurIPS_ASU,
title={Investigating the Effectiveness of Self-critiquing in {LLM}s solving Planning Tasks},
author={Karthik Valmeekam and Matthew Marquez and Subbarao Kambhampati},
booktitle={NeurIPS 2023 Foundation Models for Decision Making Workshop},
year={2023},
url={https://openreview.net/forum?id=gGQfkyb0KL}
}

@article{MustTaught_24_arXiv_NYU,
  title={Large Language Models Must Be Taught to Know What They Don't Know},
  author={Kapoor, Sanyam and Gruver, Nate and Roberts, Manley and Collins, Katherine and Pal, Arka and Bhatt, Umang and Weller, Adrian and Dooley, Samuel and Goldblum, Micah and Wilson, Andrew Gordon},
  journal={arXiv preprint arXiv:2406.08391},
  year={2024}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 方法：一致性信号获取

@article{TheoryKnowKnow_22_arXiv_Anthropic,
  title={Language models (mostly) know what they know},
  author={Kadavath, Saurav and Conerly, Tom and Askell, Amanda and Henighan, Tom and Drain, Dawn and Perez, Ethan and Schiefer, Nicholas and Hatfield-Dodds, Zac and DasSarma, Nova and Tran-Johnson, Eli and others},
  journal={arXiv preprint arXiv:2207.05221},
  year={2022}
}

@inproceedings{TheoryUncertainty_24_ICLR_NUS,
title={Can {LLM}s Express Their Uncertainty? An Empirical Evaluation of Confidence Elicitation in {LLM}s},
author={Miao Xiong and Zhiyuan Hu and Xinyang Lu and YIFEI LI and Jie Fu and Junxian He and Bryan Hooi},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=gjeQKFxFpZ}
}

@article{TheoryUncertainty_23_arXiv_UoMaryland,
  title={Quantifying uncertainty in answers from any language model and enhancing their trustworthiness},
  author={Chen, Jiuhai and Mueller, Jonas},
  journal={arXiv preprint arXiv:2308.16175},
  year={2023}
}

@article{TrustScore_24_arXiv_UoEdinburgh,
  title={TrustScore: Reference-Free Evaluation of LLM Response Trustworthiness},
  author={Zheng, Danna and Liu, Danyang and Lapata, Mirella and Pan, Jeff Z},
  journal={arXiv preprint arXiv:2402.12545},
  year={2024}
}

@inproceedings{HalluSelfCheckGPT_23_EMNLP_Cambridge,
    title = "{S}elf{C}heck{GPT}: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models",
    author = "Manakul, Potsawee  and
      Liusie, Adian  and
      Gales, Mark",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.557",
    doi = "10.18653/v1/2023.emnlp-main.557",
    pages = "9004--9017",
}

@inproceedings{INSIDE_24_ICLR_Alibaba,
title={{INSIDE}: {LLM}s' Internal States Retain the Power of Hallucination Detection},
author={Chao Chen and Kai Liu and Ze Chen and Yi Gu and Yue Wu and Mingyuan Tao and Zhihang Fu and Jieping Ye},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=Zj12nzlQbz}
}

@inproceedings{CrossExamine_23_EMNLP_TAU,
    title = "{LM} vs {LM}: Detecting Factual Errors via Cross Examination",
    author = "Cohen, Roi  and
      Hamri, May  and
      Geva, Mor  and
      Globerson, Amir",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.778",
    doi = "10.18653/v1/2023.emnlp-main.778",
    pages = "12621--12640",
}

@article{ActivePrompt_23_arXiv_HUST,
  title={Active prompting with chain-of-thought for large language models},
  author={Diao, Shizhe and Wang, Pengcheng and Lin, Yong and Zhang, Tong},
  journal={arXiv preprint arXiv:2302.12246},
  year={2023}
}

@inproceedings{Uncertainty_21_EACL_UCSB,
    title = "On Hallucination and Predictive Uncertainty in Conditional Language Generation",
    author = "Xiao, Yijun  and
      Wang, William Yang",
    editor = "Merlo, Paola  and
      Tiedemann, Jorg  and
      Tsarfaty, Reut",
    booktitle = "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume",
    month = apr,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.eacl-main.236",
    doi = "10.18653/v1/2021.eacl-main.236",
    pages = "2734--2744",
}

@article{Uncertainty_24_TMLR_Illinois,
title={Generating with Confidence: Uncertainty Quantification for Black-box Large Language Models},
author={Zhen Lin and Shubhendu Trivedi and Jimeng Sun},
journal={Transactions on Machine Learning Research},
issn={2835-8856},
year={2024},
url={https://openreview.net/forum?id=DWkJCSxKU5},
note={}
}

@article{BelieveOrNot_24_arXiv_Google,
  title={To Believe or Not to Believe Your LLM},
  author={Yadkori, Yasin Abbasi and Kuzborskij, Ilja and Gy{\"o}rgy, Andr{\'a}s and Szepesv{\'a}ri, Csaba},
  journal={arXiv preprint arXiv:2406.02543},
  year={2024}
}

@article{CriticGPT_24_arXiv_OpenAI,
  title={LLM Critics Help Catch LLM Bugs},
  author={McAleese, Nat and Pokorny, Rai Michael and Uribe, Juan Felipe Ceron and Nitishinskaya, Evgenia and Trebacz, Maja and Leike, Jan},
  journal={arXiv preprint arXiv:2407.00215},
  year={2024}
}

@article{SelfCritiquing_22_arXiv_OpenAI,
  title={Self-critiquing models for assisting human evaluators},
  author={Saunders, William and Yeh, Catherine and Wu, Jeff and Bills, Steven and Ouyang, Long and Ward, Jonathan and Leike, Jan},
  journal={arXiv preprint arXiv:2206.05802},
  year={2022}
}

@inproceedings{BARTScore_21_NeuIPS_CMU,
title={{BARTS}core: Evaluating Generated Text as Text Generation},
author={Weizhe Yuan and Graham Neubig and Pengfei Liu},
booktitle={Advances in Neural Information Processing Systems},
editor={A. Beygelzimer and Y. Dauphin and P. Liang and J. Wortman Vaughan},
year={2021},
url={https://openreview.net/forum?id=5Ya8PbvpZ9}
}

@article{GPTScore_23_arXiv_NUS,
  title={Gptscore: Evaluate as you desire},
  author={Fu, Jinlan and Ng, See-Kiong and Jiang, Zhengbao and Liu, Pengfei},
  journal={arXiv preprint arXiv:2302.04166},
  year={2023}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 方法：推理优化


@inproceedings{RealCoT_22_NeuIPS_Google,
title={Chain of Thought Prompting Elicits Reasoning in Large Language Models},
author={Jason Wei and Xuezhi Wang and Dale Schuurmans and Maarten Bosma and brian ichter and Fei Xia and Ed H. Chi and Quoc V Le and Denny Zhou},
booktitle={Advances in Neural Information Processing Systems},
editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
year={2022},
url={https://openreview.net/forum?id=_VjQlMeSB_J}
}

@inproceedings{SelfConsistency_23_ICLR_Google,
title={Self-Consistency Improves Chain of Thought Reasoning in Language Models},
author={Xuezhi Wang and Jason Wei and Dale Schuurmans and Quoc V Le and Ed H. Chi and Sharan Narang and Aakanksha Chowdhery and Denny Zhou},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=1PL1NIMMrw}
}

@inproceedings{ToT_23_NeuIPS_Princeton,
title={Tree of Thoughts: Deliberate Problem Solving with Large Language Models},
author={Shunyu Yao and Dian Yu and Jeffrey Zhao and Izhak Shafran and Thomas L. Griffiths and Yuan Cao and Karthik R Narasimhan},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
url={https://openreview.net/forum?id=5Xc1ecxO1h}
}

@article{GoT_24_AAAI_ETH, 
title={Graph of Thoughts: Solving Elaborate Problems with Large Language Models}, volume={38}, url={https://ojs.aaai.org/index.php/AAAI/article/view/29720}, DOI={10.1609/aaai.v38i16.29720}, 
number={16}, 
journal={Proceedings of the AAAI Conference on Artificial Intelligence}, 
author={Besta, Maciej and Blach, Nils and Kubicek, Ales and Gerstenberger, Robert and Podstawski, Michal and Gianinazzi, Lukas and Gajda, Joanna and Lehmann, Tomasz and Niewiadomski, Hubert and Nyczyk, Piotr and Hoefler, Torsten}, 
year={2024}, month={Mar.}, 
pages={17682-17690} 
}

@inproceedings{SelfEvaluation_23_NeuIPS_NUS,
title={Self-Evaluation Guided Beam Search for Reasoning},
author={Yuxi Xie and Kenji Kawaguchi and Yiran Zhao and Xu Zhao and Min-Yen Kan and Junxian He and Qizhe Xie},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
url={https://openreview.net/forum?id=Bw82hwg5Q3}
}

@article{CoT_24_arXiv_Google,
  title={Chain-of-thought reasoning without prompting},
  author={Wang, Xuezhi and Zhou, Denny},
  journal={arXiv preprint arXiv:2402.10200},
  year={2024}
}

@article{UniversalSelfConsistency_23_arXiv_Google,
  title={Universal self-consistency for large language model generation},
  author={Chen, Xinyun and Aksitov, Renat and Alon, Uri and Ren, Jie and Xiao, Kefan and Yin, Pengcheng and Prakash, Sushant and Sutton, Charles and Wang, Xuezhi and Zhou, Denny},
  journal={arXiv preprint arXiv:2311.17311},
  year={2023}
}

@article{SoftSelfConsistency_24_arXiv_UNCChapel,
  title={Soft Self-Consistency Improves Language Model Agents},
  author={Wang, Han and Prasad, Archiki and Stengel-Eskin, Elias and Bansal, Mohit},
  journal={arXiv preprint arXiv:2402.13212},
  year={2024}
}

@article{MPSC_23_arXiv_PKU,
  title={Enhancing large language models in coding through multi-perspective self-consistency},
  author={Huang, Baizhou and Lu, Shuai and Chen, Weizhu and Wan, Xiaojun and Duan, Nan},
  journal={arXiv preprint arXiv:2309.17272},
  year={2023}
}

@inproceedings{SelfTeach_23_ACL_PKU,
    title = "Making Language Models Better Reasoners with Step-Aware Verifier",
    author = "Li, Yifei  and
      Lin, Zeqi  and
      Zhang, Shizhuo  and
      Fu, Qiang  and
      Chen, Bei  and
      Lou, Jian-Guang  and
      Chen, Weizhu",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.291",
    doi = "10.18653/v1/2023.acl-long.291",
    pages = "5315--5333",
}

@article{Promptbreeder_23_arXiv_DeepMind,
  title={Promptbreeder: Self-referential self-improvement via prompt evolution},
  author={Fernando, Chrisantha and Banarse, Dylan and Michalewski, Henryk and Osindero, Simon and Rockt{\"a}schel, Tim},
  journal={arXiv preprint arXiv:2309.16797},
  year={2023}
}

@inproceedings{DSPy_24_ICLR_Stanford,
title={{DSP}y: Compiling Declarative Language Model Calls into State-of-the-Art Pipelines},
author={Omar Khattab and Arnav Singhvi and Paridhi Maheshwari and Zhiyuan Zhang and Keshav Santhanam and Sri Vardhamanan A and Saiful Haq and Ashutosh Sharma and Thomas T. Joshi and Hanna Moazam and Heather Miller and Matei Zaharia and Christopher Potts},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=sY5N0zY5Od}
}

@inproceedings{Maieutic_22_EMNLP_UoWashington,
    title = "Maieutic Prompting: Logically Consistent Reasoning with Recursive Explanations",
    author = "Jung, Jaehun  and
      Qin, Lianhui  and
      Welleck, Sean  and
      Brahman, Faeze  and
      Bhagavatula, Chandra  and
      Le Bras, Ronan  and
      Choi, Yejin",
    editor = "Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.82",
    doi = "10.18653/v1/2022.emnlp-main.82",
    pages = "1266--1279",
}

@article{QuietSTaR_24_arXiv_Stanford,
  title={Quiet-star: Language models can teach themselves to think before speaking},
  author={Zelikman, Eric and Harik, Georges and Shao, Yijia and Jayasiri, Varuna and Haber, Nick and Goodman, Noah D},
  journal={arXiv preprint arXiv:2403.09629},
  year={2024}
}

@inproceedings{SelfImprove_23_EMNLP_Illinois,
    title = "Large Language Models Can Self-Improve",
    author = "Huang, Jiaxin  and
      Gu, Shixiang  and
      Hou, Le  and
      Wu, Yuexin  and
      Wang, Xuezhi  and
      Yu, Hongkun  and
      Han, Jiawei",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.67",
    doi = "10.18653/v1/2023.emnlp-main.67",
    pages = "1051--1068",
}

@article{SelfImprovement_24_arXiv_Tencent,
  title={Toward Self-Improvement of LLMs via Imagination, Searching, and Criticizing},
  author={Tian, Ye and Peng, Baolin and Song, Linfeng and Jin, Lifeng and Yu, Dian and Mi, Haitao and Yu, Dong},
  journal={arXiv preprint arXiv:2404.12253},
  year={2024}
}

@article{WebAgent_24_arXiv_UPenn,
  title={Large Language Models Can Self-Improve At Web Agent Tasks},
  author={Patel, Ajay and Hofmarcher, Markus and Leoveanu-Condrei, Claudiu and Dinu, Marius-Constantin and Callison-Burch, Chris and Hochreiter, Sepp},
  journal={arXiv preprint arXiv:2405.20309},
  year={2024}
}

@article{SelfImprove_24_arXiv_Cohere,
  title={Self-Improving Robust Preference Optimization},
  author={Choi, Eugene and Ahmadian, Arash and Geist, Matthieu and Pietquin, Oilvier and Azar, Mohammad Gheshlaghi},
  journal={arXiv preprint arXiv:2406.01660},
  year={2024}
}

@inproceedings{SelfAlignment_23_NeuIPS_CMU,
 author = {Sun, Zhiqing and Shen, Yikang and Zhou, Qinhong and Zhang, Hongxin and Chen, Zhenfang and Cox, David and Yang, Yiming and Gan, Chuang},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {2511--2565},
 publisher = {Curran Associates, Inc.},
 title = {Principle-Driven Self-Alignment of Language Models from Scratch with Minimal Human Supervision},
 url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/0764db1151b936aca59249e2c1386101-Paper-Conference.pdf},
 volume = {36},
 year = {2023}
}

@misc{NLIConsistency_22_CS224N_Stanford,
  title={Improving Logical Consistency in Pre-Trained Language Models using Natural Language Inference},
  author={Agarwal, Ananth and Tzen, Anthony and Tew, Cameron},
  year={2022},
  howpublished={Stanford CS224N Custom Project},
  institution={Stanford University},
  url={https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1224/reports/custom_116994635.pdf}
}

@inproceedings{ConCoRD_22_EMNLP_Stanford,
    title = "Enhancing Self-Consistency and Performance of Pre-Trained Language Models through Natural Language Inference",
    author = "Mitchell, Eric  and
      Noh, Joseph  and
      Li, Siyan  and
      Armstrong, Will  and
      Agarwal, Ananth  and
      Liu, Patrick  and
      Finn, Chelsea  and
      Manning, Christopher",
    editor = "Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.115",
    doi = "10.18653/v1/2022.emnlp-main.115",
    pages = "1754--1768",
}

@article{LearnMistake_24_arXiv_MS,
  title={Learning from mistakes makes llm better reasoner},
  author={An, Shengnan and Ma, Zexiong and Lin, Zeqi and Zheng, Nanning and Lou, Jian-Guang and Chen, Weizhu},
  journal={arXiv preprint arXiv:2310.20689},
  year={2023}
}

@article{LearnMistake_24_arXiv_UCSD,
  title={Can LLMs Learn from Previous Mistakes? Investigating LLMs' Errors to Boost for Reasoning},
  author={Tong, Yongqi and Li, Dawei and Wang, Sizhe and Wang, Yujia and Teng, Fei and Shang, Jingbo},
  journal={arXiv preprint arXiv:2403.20046},
  year={2024}
}

@article{Debate_23_arXiv_MIT,
  title={Improving factuality and reasoning in language models through multiagent debate},
  author={Du, Yilun and Li, Shuang and Torralba, Antonio and Tenenbaum, Joshua B and Mordatch, Igor},
  journal={arXiv preprint arXiv:2305.14325},
  year={2023}
}

@inproceedings{ModalCollaboration_23_EMNLP_HIT,
    title = "Examining Inter-Consistency of Large Language Models Collaboration: An In-depth Analysis via Debate",
    author = "Xiong, Kai  and
      Ding, Xiao  and
      Cao, Yixin  and
      Liu, Ting  and
      Qin, Bing",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2023",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-emnlp.508",
    doi = "10.18653/v1/2023.findings-emnlp.508",
    pages = "7572--7590",
}

@inproceedings{REFINER_24_EACL_EPFL,
    title = "{REFINER}: Reasoning Feedback on Intermediate Representations",
    author = "Paul, Debjit  and
      Ismayilzada, Mete  and
      Peyrard, Maxime  and
      Borges, Beatriz  and
      Bosselut, Antoine  and
      West, Robert  and
      Faltings, Boi",
    editor = "Graham, Yvette  and
      Purver, Matthew",
    booktitle = "Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = mar,
    year = "2024",
    address = "St. Julian{'}s, Malta",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.eacl-long.67",
    pages = "1100--1126",
}

@inproceedings{ConsensusGame_24_ICLR_MIT,
title={The Consensus Game: Language Model Generation via Equilibrium Search},
author={Athul Paul Jacob and Yikang Shen and Gabriele Farina and Jacob Andreas},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=n9xeGcI4Yg}
}

@article{MACNet_24_arXiv_THU,
  title={Scaling Large-Language-Model-based Multi-Agent Collaboration},
  author={Qian, Chen and Xie, Zihao and Wang, Yifei and Liu, Wei and Dang, Yufan and Du, Zhuoyun and Chen, Weize and Yang, Cheng and Liu, Zhiyuan and Sun, Maosong},
  journal={arXiv preprint arXiv:2406.07155},
  year={2024}
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 方法：幻觉缓解

@inproceedings{SelfRefine_23_NeuIPS_CMU,
title={Self-Refine: Iterative Refinement with Self-Feedback},
author={Aman Madaan and Niket Tandon and Prakhar Gupta and Skyler Hallinan and Luyu Gao and Sarah Wiegreffe and Uri Alon and Nouha Dziri and Shrimai Prabhumoye and Yiming Yang and Shashank Gupta and Bodhisattwa Prasad Majumder and Katherine Hermann and Sean Welleck and Amir Yazdanbakhsh and Peter Clark},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
url={https://openreview.net/forum?id=S37hOerQLB}
}

@inproceedings{Reflexion_23_NeuIPS_Northeastern,
 author = {Shinn, Noah and Cassano, Federico and Gopinath, Ashwin and Narasimhan, Karthik and Yao, Shunyu},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {8634--8652},
 publisher = {Curran Associates, Inc.},
 title = {Reflexion: language agents with verbal reinforcement learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/1b44b878bb782e6954cd888628510e90-Paper-Conference.pdf},
 volume = {36},
 year = {2023}
}

@inproceedings{SelfCorrect_23_ICLR_AI2,
title={Generating Sequences by Learning to Self-Correct},
author={Sean Welleck and Ximing Lu and Peter West and Faeze Brahman and Tianxiao Shen and Daniel Khashabi and Yejin Choi},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=hH36JeQZDaO}
}

@inproceedings{Re3_23_EMNLP_Berkeley,
    title = "Re3: Generating Longer Stories With Recursive Reprompting and Revision",
    author = "Yang, Kevin  and
      Tian, Yuandong  and
      Peng, Nanyun  and
      Klein, Dan",
    editor = "Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.296",
    doi = "10.18653/v1/2022.emnlp-main.296",
    pages = "4393--4479",
}

@inproceedings{PEER_23_ICLR_Meta,
title={{PEER}: A Collaborative Language Model},
author={Timo Schick and Jane A. Yu and Zhengbao Jiang and Fabio Petroni and Patrick Lewis and Gautier Izacard and Qingfei You and Christoforos Nalmpantis and Edouard Grave and Sebastian Riedel},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=KbYevcLjnc}
}

@inproceedings{SelfDebug_24_ICLR_Google,
title={Teaching Large Language Models to Self-Debug},
author={Xinyun Chen and Maxwell Lin and Nathanael Sch{\"a}rli and Denny Zhou},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=KuPixIqPiq}
}

@inproceedings{MAF_23_EMNLP_UCSB,
title={{MAF}: Multi-Aspect Feedback for Improving Reasoning in Large Language Models},
author={Deepak Nathani and David Wang and Liangming Pan and William Yang Wang},
booktitle={The 2023 Conference on Empirical Methods in Natural Language Processing},
year={2023},
url={https://openreview.net/forum?id=bNeDLx5O6w}
}

@article{SelfBias_24_arXiv_UCSB,
  title={Pride and Prejudice: LLM Amplifies Self-Bias in Self-Refinement},
  year={2024},
  journal={arXiv preprint arXiv:2402.11436},
  author={Xu, Wenda and Zhu, Guanglei and Zhao, Xuandong and Pan, Liangming and Li, Lei and Wang, William Yang}
}

@article{FAVA_24_arXiv_Washington,
  title={Fine-grained hallucination detection and editing for language models},
  author={Mishra, Abhika and Asai, Akari and Balachandran, Vidhisha and Wang, Yizhong and Neubig, Graham and Tsvetkov, Yulia and Hajishirzi, Hannaneh},
  journal={arXiv preprint arXiv:2401.06855},
  year={2024}
}

@article{EVER_arXiv_23_UNC,
  title={Ever: Mitigating hallucination in large language models through real-time verification and rectification},
  author={Kang, Haoqiang and Ni, Juntong and Yao, Huaxiu},
  journal={arXiv preprint arXiv:2311.09114},
  year={2023}
}

@inproceedings{HalluSelfContradictory_24_ICLR_ETH,
title={Self-contradictory Hallucinations of Large Language Models: Evaluation, Detection and Mitigation},
author={Niels M{\"u}ndler and Jingxuan He and Slobodan Jenko and Martin Vechev},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=EmQSOi1X2f}
}

@article{PURR_23_arXiv_UCI,
  title={Purr: Efficiently editing language model hallucinations by denoising language model corruptions},
  author={Chen, Anthony and Pasupat, Panupong and Singh, Sameer and Lee, Hongrae and Guu, Kelvin},
  journal={arXiv preprint arXiv:2305.14908},
  year={2023}
}

@inproceedings{RARR_23_ACL_CMU,
    title = "{RARR}: Researching and Revising What Language Models Say, Using Language Models",
    author = "Gao, Luyu  and
      Dai, Zhuyun  and
      Pasupat, Panupong  and
      Chen, Anthony  and
      Chaganty, Arun Tejasvi  and
      Fan, Yicheng  and
      Zhao, Vincent  and
      Lao, Ni  and
      Lee, Hongrae  and
      Juan, Da-Cheng  and
      Guu, Kelvin",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.910",
    doi = "10.18653/v1/2023.acl-long.910",
    pages = "16477--16508",
}

@inproceedings{CCS_23_ICLR_UCB,
title={Discovering Latent Knowledge in Language Models Without Supervision},
author={Collin Burns and Haotian Ye and Dan Klein and Jacob Steinhardt},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=ETKGuby0hcs}
}

@inproceedings{ITI_23_NeuIPS_Harvard,
title={Inference-Time Intervention: Eliciting Truthful Answers from a Language Model},
author={Kenneth Li and Oam Patel and Fernanda Vi{\'e}gas and Hanspeter Pfister and Martin Wattenberg},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
url={https://openreview.net/forum?id=aLLuYpn83y}
}

@article{TrFr_24_AAAI_BUAA, 
title={Truth Forest: Toward Multi-Scale Truthfulness in Large Language Models through Intervention without Tuning}, 
volume={38}, 
url={https://ojs.aaai.org/index.php/AAAI/article/view/30087}, 
DOI={10.1609/aaai.v38i19.30087}, 
number={19}, 
journal={Proceedings of the AAAI Conference on Artificial Intelligence}, 
author={Chen, Zhongzhi and Sun, Xingwu and Jiao, Xianfeng and Lian, Fengzong and Kang, Zhanhui and Wang, Di and Xu, Chengzhong}, 
year={2024}, 
month={Mar.}, 
pages={20967-20974} 
}

@inproceedings{TruthX_24_ACL_ICT,
    title={TruthX: Alleviating Hallucinations by Editing Large Language Models in Truthful Space}, 
    author={Shaolei Zhang and Tian Yu and Yang Feng},
    year={2024},
    url={https://arxiv.org/abs/2402.17811},
    booktitle = {Proceedings of the 62th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
    publisher = {Association for Computational Linguistics},
}

@article{RetrievalHead_24_arXiv_PKU,
  title={Retrieval head mechanistically explains long-context factuality},
  author={Wu, Wenhao and Wang, Yizhong and Xiao, Guangxuan and Peng, Hao and Fu, Yao},
  journal={arXiv preprint arXiv:2404.15574},
  year={2024}
}

@article{FactualFT_23_arXiv_Stanford,
  title={Fine-tuning language models for factuality},
  author={Tian, Katherine and Mitchell, Eric and Yao, Huaxiu and Manning, Christopher D and Finn, Chelsea},
  journal={arXiv preprint arXiv:2311.08401},
  year={2023}
}

@article{CD_22_arXiv_Stanford,
  title={Contrastive decoding: Open-ended text generation as optimization},
  author={Li, Xiang Lisa and Holtzman, Ari and Fried, Daniel and Liang, Percy and Eisner, Jason and Hashimoto, Tatsunori and Zettlemoyer, Luke and Lewis, Mike},
  journal={arXiv preprint arXiv:2210.15097},
  year={2022}
}

@article{ContextAwareD_23_arXiv_Washington,
  title={Trusting your evidence: Hallucinate less with context-aware decoding},
  author={Shi, Weijia and Han, Xiaochuang and Lewis, Mike and Tsvetkov, Yulia and Zettlemoyer, Luke and Yih, Scott Wen-tau},
  journal={arXiv preprint arXiv:2305.14739},
  year={2023}
}

@inproceedings{DoLa_24_ICLR_MIT,
    title={DoLa: Decoding by Contrasting Layers Improves Factuality in Large Language Models},
    author={Yung-Sung Chuang and Yujia Xie and Hongyin Luo and Yoon Kim and James R. Glass and Pengcheng He},
    booktitle={The Twelfth International Conference on Learning Representations},
    year={2024},
    url={https://openreview.net/forum?id=Th6NyL07na}
}

@article{FastMem_24_arXiv_KUL,
  title={FastMem: Fast Memorization of Prompt Improves Context Awareness of Large Language Models},
  author={Zhu, Junyi and Liu, Shuochen and Yu, Yu and Tang, Bo and Yan, Yibo and Li, Zhiyu and Xiong, Feiyu and Xu, Tong and Blaschko, Matthew B},
  journal={arXiv preprint arXiv:2406.16069},
  year={2024}
}

@article{DIVER_24_arXiv_IA,
  title={Diver: Large Language Model Decoding with Span-Level Mutual Information Verification},
  author={Lu, Jinliang and Wang, Chen and Zhang, Jiajun},
  journal={arXiv preprint arXiv:2406.02120},
  year={2024}
}

@article{SED_24_arXiv_FDU,
  title={SED: Self-Evaluation Decoding Enhances Large Language Models for Better Generation},
  author={Luo, Ziqin and Han, Haixia and Zhao, Haokun and Jiang, Guochao and Du, Chengyu and Li, Tingyun and Liang, Jiaqing and Yang, Deqing and Xiao, Yanghua},
  journal={arXiv preprint arXiv:2405.16552},
  year={2024}
}

@article{ContextDecode_24_arXiv_Edin,
  title={Enhancing Contextual Understanding in Large Language Models through Contrastive Decoding},
  author={Zhao, Zheng and Monti, Emilio and Lehmann, Jens and Assem, Haytham},
  journal={arXiv preprint arXiv:2405.02750},
  year={2024}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 方法：Other tasks

@inproceedings{TheoryKnowUnknown_23_ACL_Fudan,
    title = "Do Large Language Models Know What They Don{'}t Know?",
    author = "Yin, Zhangyue  and
      Sun, Qiushi  and
      Guo, Qipeng  and
      Wu, Jiawen  and
      Qiu, Xipeng  and
      Huang, Xuanjing",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2023",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-acl.551",
    doi = "10.18653/v1/2023.findings-acl.551",
    pages = "8653--8665",
}

@article{TheoryKnowUnknown_24_arxiv_Fudan,
  title={Can AI Assistants Know What They Don't Know?},
  author={Cheng, Qinyuan and Sun, Tianxiang and Liu, Xiangyang and Zhang, Wenwei and Yin, Zhangyue and Li, Shimin and Li, Linyang and Chen, Kai and Qiu, Xipeng},
  journal={arXiv preprint arXiv:2401.13275},
  year={2024}
}

@inproceedings{SelfKnowledge_19_RANLP_Handong,
    title = "Self-Knowledge Distillation in Natural Language Processing",
    author = "Hahn, Sangchul  and
      Choi, Heeyoul",
    editor = "Mitkov, Ruslan  and
      Angelova, Galia",
    booktitle = "Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2019)",
    month = sep,
    year = "2019",
    address = "Varna, Bulgaria",
    publisher = "INCOMA Ltd.",
    url = "https://aclanthology.org/R19-1050",
    doi = "10.26615/978-954-452-056-4_050",
    pages = "423--430"
}

@inproceedings{GKD_24_ICLR_Google,
title={On-Policy Distillation of Language Models: Learning from Self-Generated Mistakes},
author={Rishabh Agarwal and Nino Vieillard and Yongchao Zhou and Piotr Stanczyk and Sabela Ramos Garea and Matthieu Geist and Olivier Bachem},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=3zKtaqxLhW}
}

@inproceedings{SelfInstruct_23_ACL_Washington,
    title = "Self-Instruct: Aligning Language Models with Self-Generated Instructions",
    author = "Wang, Yizhong  and
      Kordi, Yeganeh  and
      Mishra, Swaroop  and
      Liu, Alisa  and
      Smith, Noah A.  and
      Khashabi, Daniel  and
      Hajishirzi, Hannaneh",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.754",
    doi = "10.18653/v1/2023.acl-long.754",
    pages = "13484--13508"
}

@inproceedings{TFKD_20_CVPR_NUS,
  title={Revisiting knowledge distillation via label smoothing regularization},
  author={Yuan, Li and Tay, Francis EH and Li, Guilin and Wang, Tao and Feng, Jiashi},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={3903--3911},
  year={2020}
}

@inproceedings{PSKD_21_ICCV_LG,
  title={Self-knowledge distillation with progressive refinement of targets},
  author={Kim, Kyungyul and Ji, ByeongMoon and Yoon, Doyoung and Hwang, Sangheum},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={6567--6576},
  year={2021}
}

@article{RLAIF_22_arXiv_anthropic,
  title={Constitutional ai: Harmlessness from ai feedback},
  author={Bai, Yuntao and Kadavath, Saurav and Kundu, Sandipan and Askell, Amanda and Kernion, Jackson and Jones, Andy and Chen, Anna and Goldie, Anna and Mirhoseini, Azalia and McKinnon, Cameron and others},
  journal={arXiv preprint arXiv:2212.08073},
  year={2022}
}

@article{Selfee_23_blog,
  title={Selfee: Iterative self-revising llm empowered by self-feedback generation},
  author={Ye, Seonghyeon and Jo, Yongrae and Kim, Doyoung and Kim, Sungdong and Hwang, Hyeonbin and Seo, Minjoon},
  journal={Blog post},
  year={2023}
}

@inproceedings{PERsD_23_EMNLP_NTU,
title={Personalized Distillation: Empowering Open-Sourced {LLM}s with Adaptive Learning for Code Generation},
author={Hailin Chen and Amrita Saha and Steven Hoi and Shafiq Joty},
booktitle={The 2023 Conference on Empirical Methods in Natural Language Processing},
year={2023},
url={https://openreview.net/forum?id=alxWMBcNVN}
}

@inproceedings{FIGA_24_ICLR_RUC,
title={Beyond Imitation: Leveraging Fine-grained Quality Signals for Alignment},
author={Geyang Guo and Ranchi Zhao and Tianyi Tang and Xin Zhao and Ji-Rong Wen},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=LNLjU5C5dK}
}

@inproceedings{MiniLLM_24_ICLR_THU,
title={Mini{LLM}: Knowledge Distillation of Large Language Models},
author={Yuxian Gu and Li Dong and Furu Wei and Minlie Huang},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=5h0qf7IBZZ}
}

@article{ImposDistill_23_arXiv_UoW,
  title={Impossible distillation: from low-quality model to high-quality dataset \& model for summarization and paraphrasing},
  author={Jung, Jaehun and West, Peter and Jiang, Liwei and Brahman, Faeze and Lu, Ximing and Fisher, Jillian and Sorensen, Taylor and Choi, Yejin},
  journal={arXiv preprint arXiv:2305.16635},
  year={2023}
}

@article{ReST_23_arXiv_Google,
  title={Reinforced self-training (rest) for language modeling},
  author={Gulcehre, Caglar and Paine, Tom Le and Srinivasan, Srivatsan and Konyushkova, Ksenia and Weerts, Lotte and Sharma, Abhishek and Siddhant, Aditya and Ahern, Alex and Wang, Miaosen and Gu, Chenjie and others},
  journal={arXiv preprint arXiv:2308.08998},
  year={2023}
}

@article{HH-RLHF_22_arXiv_antropic,
  title={Training a helpful and harmless assistant with reinforcement learning from human feedback},
  author={Bai, Yuntao and Jones, Andy and Ndousse, Kamal and Askell, Amanda and Chen, Anna and DasSarma, Nova and Drain, Dawn and Fort, Stanislav and Ganguli, Deep and Henighan, Tom and others},
  journal={arXiv preprint arXiv:2204.05862},
  year={2022}
}

@inproceedings{OASST_23_NIPS_XX,
 author = {K\"{o}pf, Andreas and Kilcher, Yannic and von R\"{u}tte, Dimitri and Anagnostidis, Sotiris and Tam, Zhi Rui and Stevens, Keith and Barhoum, Abdullah and Nguyen, Duc and Stanley, Oliver and Nagyfi, Rich\'{a}rd and ES, Shahul and Suri, Sameer and Glushkov, David and Dantuluri, Arnav and Maguire, Andrew and Schuhmann, Christoph and Nguyen, Huu and Mattick, Alexander},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {47669--47681},
 publisher = {Curran Associates, Inc.},
 title = {OpenAssistant Conversations - Democratizing Large Language Model Alignment},
 url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/949f0f8f32267d297c2d4e3ee10a2e7e-Paper-Datasets_and_Benchmarks.pdf},
 volume = {36},
 year = {2023}
}

@inproceedings{BeaverTails_23_NIPS_PKU,
 author = {Ji, Jiaming and Liu, Mickel and Dai, Josef and Pan, Xuehai and Zhang, Chi and Bian, Ce and Chen, Boyuan and Sun, Ruiyang and Wang, Yizhou and Yang, Yaodong},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {24678--24704},
 publisher = {Curran Associates, Inc.},
 title = {BeaverTails: Towards Improved Safety Alignment of LLM via a Human-Preference Dataset},
 url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/4dbb61cb68671edc4ca3712d70083b9f-Paper-Datasets_and_Benchmarks.pdf},
 volume = {36},
 year = {2023}
}

@inproceedings{SafeRLHF_23_arXiv_PKU,
title={Safe {RLHF}: Safe Reinforcement Learning from Human Feedback},
author={Josef Dai and Xuehai Pan and Ruiyang Sun and Jiaming Ji and Xinbo Xu and Mickel Liu and Yizhou Wang and Yaodong Yang},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=TyFrPOKYXw}
}

@article{ConstitutionalAI_22_arXiv_anthropic,
  title={Constitutional ai: Harmlessness from ai feedback},
  author={Bai, Yuntao and Kadavath, Saurav and Kundu, Sandipan and Askell, Amanda and Kernion, Jackson and Jones, Andy and Chen, Anna and Goldie, Anna and Mirhoseini, Azalia and McKinnon, Cameron and others},
  journal={arXiv preprint arXiv:2212.08073},
  year={2022}
}

@inproceedings{SALMON_24_ICLR_IBM,
    title={{SALMON}: Self-Alignment with Instructable Reward Models},
    author={Zhiqing Sun and Yikang Shen and Hongxin Zhang and Qinhong Zhou and Zhenfang Chen and David Daniel Cox and Yiming Yang and Chuang Gan},
    booktitle={The Twelfth International Conference on Learning Representations},
    year={2024},
    url={https://openreview.net/forum?id=xJbsmB8UMx}
}


@InProceedings{DataDifficult_22_PMLR_UoW,
  title = 	 {Understanding Dataset Difficulty with $\mathcal{V}$-Usable Information},
  author =       {Ethayarajh, Kawin and Choi, Yejin and Swayamdipta, Swabha},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {5988--6008},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/ethayarajh22a/ethayarajh22a.pdf},
  url = 	 {https://proceedings.mlr.press/v162/ethayarajh22a.html}
}

@article{yu2024xfinder,
  title={xFinder: Robust and Pinpoint Answer Extraction for Large Language Models},
  author={Yu, Qingchen and Zheng, Zifan and Song, Shichao and Li, Zhiyu and Xiong, Feiyu and Tang, Bo and Chen, Ding},
  journal={arXiv preprint arXiv:2405.11874},
  year={2024}
}

@article{ALMoST_23_arXiv_NAVER,
  title={Aligning large language models through synthetic feedback},
  author={Kim, Sungdong and Bae, Sanghwan and Shin, Jamin and Kang, Soyoung and Kwak, Donghyun and Yoo, Kang Min and Seo, Minjoon},
  journal={arXiv preprint arXiv:2305.13735},
  year={2023}
}

@article{SelfCritique_24_arXiv_Zhipu,
  title={ChatGLM-Math: Improving Math Problem-Solving in Large Language Models with a Self-Critique Pipeline},
  author={Xu, Yifan and Liu, Xiao and Liu, Xinghan and Hou, Zhenyu and Li, Yueyan and Zhang, Xiaohan and Wang, Zihan and Zeng, Aohan and Du, Zhengxiao and Zhao, Wenyi and others},
  journal={arXiv preprint arXiv:2404.02893},
  year={2024}
}

@article{InstructGPT,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={27730--27744},
  year={2022}
}

@article{SurveyKD_24_arXiv_HKU,
  title={A survey on knowledge distillation of large language models},
  author={Xu, Xiaohan and Li, Ming and Tao, Chongyang and Shen, Tao and Cheng, Reynold and Li, Jinyang and Xu, Can and Tao, Dacheng and Zhou, Tianyi},
  journal={arXiv preprint arXiv:2402.13116},
  year={2024}
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 仅引用而已，不需要下载

@article{LLMCheater_23_arXiv_RUC,
  title={Don't Make Your LLM an Evaluation Benchmark Cheater},
  author={Zhou, Kun and Zhu, Yutao and Chen, Zhipeng and Chen, Wentong and Zhao, Wayne Xin and Chen, Xu and Lin, Yankai and Wen, Ji-Rong and Han, Jiawei},
  journal={arXiv preprint arXiv:2311.01964},
  year={2023}
}

@article{PromptSensitive_23_arXiv_UoW,
  title={Quantifying Language Models' Sensitivity to Spurious Features in Prompt Design or: How I learned to start worrying about prompt formatting},
  author={Sclar, Melanie and Choi, Yejin and Tsvetkov, Yulia and Suhr, Alane},
  journal={arXiv preprint arXiv:2310.11324},
  year={2023}
}

@book{Tarski_1941, 
place={New York, United States}, 
title={Introduction to logic: And to the methodology of deductive sciences}, publisher={Oxford University Press}, 
author={Tarski, Alfred}, 
year={1941}
}

@inproceedings{DINO,
    title = "Generating Datasets with Pretrained Language Models",
    author = {Schick, Timo  and
      Sch{\"u}tze, Hinrich},
    editor = "Moens, Marie-Francine  and
      Huang, Xuanjing  and
      Specia, Lucia  and
      Yih, Scott Wen-tau",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.555",
    doi = "10.18653/v1/2021.emnlp-main.555",
    pages = "6943--6951",
}

@article{han2024llm,
  title={LLM multi-agent systems: Challenges and open problems},
  author={Han, Shanshan and Zhang, Qifan and Yao, Yuhang and Jin, Weizhao and Xu, Zhaozhuo and He, Chaoyang},
  journal={arXiv preprint arXiv:2402.03578},
  year={2024}
}

@inproceedings{deng2023uncertainty,
  title={Uncertainty estimation by fisher information-based evidential deep learning},
  author={Deng, Danruo and Chen, Guangyong and Yu, Yang and Liu, Furui and Heng, Pheng-Ann},
  booktitle={International Conference on Machine Learning},
  pages={7596--7616},
  year={2023},
  organization={PMLR}
}

@article{ignatiev2019rc2,
  title={{RC2: An Efficient MaxSAT Solver}},
  author={Ignatiev, Alexey and Morgado, Antonio and Marques-Silva, Joao},
  journal={Journal on Satisfiability, Boolean Modeling and Computation},
  volume={11},
  pages={53--64},
  year={2019},
  month={January},
  note={Published: 1 Jan. 2019}
}

@Inbook{Battiti2009,
author="Battiti, Roberto",
editor="Floudas, Christodoulos A.
and Pardalos, Panos M.",
title="Maximum satisfiability problemMaximum Satisfiability Problem",
bookTitle="Encyclopedia of Optimization",
year="2009",
publisher="Springer US",
address="Boston, MA",
pages="2035--2041",
abstract="Keywords",
isbn="978-0-387-74759-0",
doi="10.1007/978-0-387-74759-0_364",
url="https://doi.org/10.1007/978-0-387-74759-0_364"
}

@article{liang2023uhgeval,
  title={Uhgeval: Benchmarking the hallucination of chinese large language models via unconstrained generation},
  author={Liang, Xun and Song, Shichao and Niu, Simin and Li, Zhiyu and Xiong, Feiyu and Tang, Bo and Wy, Zhaohui and He, Dawei and Cheng, Peng and Wang, Zhonghao and others},
  journal={arXiv preprint arXiv:2311.15296},
  year={2023}
}

@article{nezhurina2024alice,
  title={Alice in Wonderland: Simple Tasks Showing Complete Reasoning Breakdown in State-Of-the-Art Large Language Models},
  author={Nezhurina, Marianna and Cipolina-Kun, Lucia and Cherti, Mehdi and Jitsev, Jenia},
  journal={arXiv preprint arXiv:2406.02061},
  year={2024}
}

@article{templeton2024scaling,
       title={Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet},
       author={Templeton, Adly and Conerly, Tom and Marcus, Jonathan and Lindsey, Jack and Bricken, Trenton and Chen, Brian and Pearce, Adam and Citro, Craig and Ameisen, Emmanuel and Jones, Andy and Cunningham, Hoagy and Turner, Nicholas L and McDougall, Callum and MacDiarmid, Monte and Freeman, C. Daniel and Sumers, Theodore R. and Rees, Edward and Batson, Joshua and Jermyn, Adam and Carter, Shan and Olah, Chris and Henighan, Tom},
       year={2024},
       journal={Transformer Circuits Thread},
       url={https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html}
}

@article{gao2024scaling,
  title={Scaling and evaluating sparse autoencoders},
  author={Gao, Leo and la Tour, Tom Dupr{\'e} and Tillman, Henk and Goh, Gabriel and Troll, Rajan and Radford, Alec and Sutskever, Ilya and Leike, Jan and Wu, Jeffrey},
  journal={arXiv preprint arXiv:2406.04093},
  year={2024}
}

@article{zhao2023survey,
  title={A survey of large language models},
  author={Zhao, Wayne Xin and Zhou, Kun and Li, Junyi and Tang, Tianyi and Wang, Xiaolei and Hou, Yupeng and Min, Yingqian and Zhang, Beichen and Zhang, Junjie and Dong, Zican and others},
  journal={arXiv preprint arXiv:2303.18223},
  year={2023}
}

@article{mondorf2024accuracy,
  title={Beyond Accuracy: Evaluating the Reasoning Behavior of Large Language Models--A Survey},
  author={Mondorf, Philipp and Plank, Barbara},
  journal={arXiv preprint arXiv:2404.01869},
  year={2024}
}

@article{zhang2024llm,
  title={LLM as a Mastermind: A Survey of Strategic Reasoning with Large Language Models},
  author={Zhang, Yadong and Mao, Shaoguang and Ge, Tao and Wang, Xun and de Wynter, Adrian and Xia, Yan and Wu, Wenshan and Song, Ting and Lan, Man and Wei, Furu},
  journal={arXiv preprint arXiv:2404.01230},
  year={2024}
}

@article{zhang2023sirens,
  title={Siren's song in the AI ocean: a survey on hallucination in large language models},
  author={Zhang, Yue and Li, Yafu and Cui, Leyang and Cai, Deng and Liu, Lemao and Fu, Tingchen and Huang, Xinting and Zhao, Enbo and Zhang, Yu and Chen, Yulong and others},
  journal={arXiv preprint arXiv:2309.01219},
  year={2023}
}

@inproceedings{hendrycks2021measuring,
title={Measuring Massive Multitask Language Understanding},
author={Dan Hendrycks and Collin Burns and Steven Basart and Andy Zou and Mantas Mazeika and Dawn Song and Jacob Steinhardt},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=d7KBjmI3GmQ}
}

@inproceedings{lin-etal-2022-truthfulqa,
    title = "{T}ruthful{QA}: Measuring How Models Mimic Human Falsehoods",
    author = "Lin, Stephanie  and
      Hilton, Jacob  and
      Evans, Owain",
    editor = "Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.229",
    doi = "10.18653/v1/2022.acl-long.229",
    pages = "3214--3252",
}

@article{hillier2024super,
  title={Super Tiny Language Models},
  author={Hillier, Dylan and Guertler, Leon and Tan, Cheston and Agrawal, Palaash and Ruirui, Chen and Cheng, Bobby},
  journal={arXiv preprint arXiv:2405.14159},
  year={2024}
}

@article{kaplan2020scaling,
  title={Scaling laws for neural language models},
  author={Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  journal={arXiv preprint arXiv:2001.08361},
  year={2020}
}

@article{RATT_24_arXiv_PSU,
  title={RATT: AThought Structure for Coherent and Correct LLMReasoning},
  author={Zhang, Jinghan and Wang, Xiting and Ren, Weijieying and Jiang, Lu and Wang, Dongjie and Liu, Kunpeng},
  journal={arXiv preprint arXiv:2406.02746},
  year={2024}
}

@inproceedings{joshi2017triviaqa,
    title = "{T}rivia{QA}: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension",
    author = "Joshi, Mandar  and
      Choi, Eunsol  and
      Weld, Daniel  and
      Zettlemoyer, Luke",
    editor = "Barzilay, Regina  and
      Kan, Min-Yen",
    booktitle = "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2017",
    address = "Vancouver, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P17-1147",
    doi = "10.18653/v1/P17-1147",
    pages = "1601--1611",
}

@inproceedings{sun2024evaluating,
title={Evaluating the Zero-shot Robustness of Instruction-tuned Language Models},
author={Jiuding Sun and Chantal Shaib and Byron C Wallace},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=g9diuvxN6D}
}

@inproceedings{FFLM_23_NIPS_CMU,
title={Exposing Attention Glitches with Flip-Flop Language Modeling},
author={Bingbin Liu and Jordan T. Ash and Surbhi Goel and Akshay Krishnamurthy and Cyril Zhang},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
url={https://openreview.net/forum?id=VzmpXQAn6E}
}

@article{LiTM_24_TACL_Stanford,
  title={Lost in the middle: How language models use long contexts},
  author={Liu, Nelson F and Lin, Kevin and Hewitt, John and Paranjape, Ashwin and Bevilacqua, Michele and Petroni, Fabio and Liang, Percy},
  journal={Transactions of the Association for Computational Linguistics},
  volume={12},
  pages={157--173},
  year={2024},
  publisher={MIT Press One Broadway, 12th Floor, Cambridge, Massachusetts 02142, USA~…}
}

@article{cobbe2021training,
  title={Training verifiers to solve math word problems},
  author={Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and others},
  journal={arXiv preprint arXiv:2110.14168},
  year={2021}
}

@article{PrincipleSC_22_FITEE,
  title={On the principles of parsimony and self-consistency for the emergence of intelligence},
  author={Ma, Yi and Tsao, Doris and Shum, Heung-Yeung},
  journal={Frontiers of Information Technology \& Electronic Engineering},
  volume={23},
  number={9},
  pages={1298--1323},
  year={2022},
  publisher={Springer}
}

@inproceedings{Parrots_21_FAccT_UoW,
  title={On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?},
  author={Bender, Emily M and Gebru, Timnit and McMillan-Major, Angelina and Shmitchell, Shmargaret},
  booktitle={Proceedings of the 2021 ACM conference on fairness, accountability, and transparency},
  pages={610--623},
  year={2021}
}


@InProceedings{pmlr-v48-gal16,
  title = 	 {Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning},
  author = 	 {Gal, Yarin and Ghahramani, Zoubin},
  booktitle = 	 {Proceedings of The 33rd International Conference on Machine Learning},
  pages = 	 {1050--1059},
  year = 	 {2016},
  editor = 	 {Balcan, Maria Florina and Weinberger, Kilian Q.},
  volume = 	 {48},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {New York, New York, USA},
  month = 	 {20--22 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v48/gal16.pdf},
  url = 	 {https://proceedings.mlr.press/v48/gal16.html},
}

@InProceedings{genetic_algorithm,
author="Harvey, Inman",
editor="Kampis, George
and Karsai, Istv{\'a}n
and Szathm{\'a}ry, E{\"o}rs",
title="The Microbial Genetic Algorithm",
booktitle="Advances in Artificial Life. Darwin Meets von Neumann",
year="2011",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="126--133",
abstract="We analyse how the conventional Genetic Algorithm can be stripped down and reduced to its basics. We present a minimal, modified version that can be interpreted in terms of horizontal gene transfer, as in bacterial conjugation. Whilst its functionality is effectively similar to the conventional version, it is much easier to program, and recommended for both teaching purposes and practical applications. Despite the simplicity of the core code, it effects Selection, (variable rates of) Recombination, Mutation, Elitism (`for free') and Geographical Distribution.",
isbn="978-3-642-21314-4"
}

@article{PPO_17_arXiv_OpenAI,
  title={Proximal policy optimization algorithms},
  author={Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  journal={arXiv preprint arXiv:1707.06347},
  year={2017}
}

@inproceedings{DPO_23_NIPS_Stanford,
 author = {Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Manning, Christopher D and Ermon, Stefano and Finn, Chelsea},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {53728--53741},
 publisher = {Curran Associates, Inc.},
 title = {Direct Preference Optimization: Your Language Model is Secretly a Reward Model},
 url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/a85b405ed65c6477a4fe8302b5e06ce7-Paper-Conference.pdf},
 volume = {36},
 year = {2023}
}

@article{ConsisEval_24_arXiv_PKU,
  title={Can Large Language Models Always Solve Easy Problems if They Can Solve Harder Ones?},
  author={Yang, Zhe and Zhang, Yichang and Liu, Tianyu and Yang, Jian and Lin, Junyang and Zhou, Chang and Sui, Zhifang},
  journal={arXiv preprint arXiv:2406.12809},
  year={2024}
}

@article{SurveyPL_24_arXiv_HIT,
  title={A Survey on Human Preference Learning for Large Language Models},
  author={Jiang, Ruili and Chen, Kehai and Bai, Xuefeng and He, Zhixuan and Li, Juntao and Yang, Muyun and Zhao, Tiejun and Nie, Liqiang and Zhang, Min},
  journal={arXiv preprint arXiv:2406.11191},
  year={2024}
}

@article{ExpressKnowledge_24_arXiv_FDU,
  title={Teaching Large Language Models to Express Knowledge Boundary from Their Own Signals},
  author={Chen, Lida and Liang, Zujie and Wang, Xintao and Liang, Jiaqing and Xiao, Yanghua and Wei, Feng and Chen, Jinglei and Hao, Zhenghong and Han, Bing and Wang, Wei},
  journal={arXiv preprint arXiv:2406.10881},
  year={2024}
}

@article{CEval,
  title={C-eval: A multi-level multi-discipline chinese evaluation suite for foundation models},
  author={Huang, Yuzhen and Bai, Yuzhuo and Zhu, Zhihao and Zhang, Junlei and Zhang, Jinghan and Su, Tangjun and Liu, Junteng and Lv, Chuancheng and Zhang, Yikai and Fu, Yao and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{MMLU2,
  title={Aligning AI With Shared Human Values},
  author={Dan Hendrycks and Collin Burns and Steven Basart and Andrew Critch and Jerry Li and Dawn Song and Jacob Steinhardt},
  journal={Proceedings of the International Conference on Learning Representations (ICLR)},
  year={2021}
}

@article{Bench_BBH,
  title={Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them},
  author={Suzgun, Mirac and Scales, Nathan and Sch{\"a}rli, Nathanael and Gehrmann, Sebastian and Tay, Yi and Chung, Hyung Won and Chowdhery, Aakanksha and Le, Quoc V and Chi, Ed H and Zhou, Denny and and Wei, Jason},
  journal={arXiv preprint arXiv:2210.09261},
  year={2022}
}

@article{ARC_c,
  title={Think you have solved question answering? try arc, the ai2 reasoning challenge},
  author={Clark, Peter and Cowhey, Isaac and Etzioni, Oren and Khot, Tushar and Sabharwal, Ashish and Schoenick, Carissa and Tafjord, Oyvind},
  journal={arXiv preprint arXiv:1803.05457},
  year={2018}
}


@article{bench_math,
  title={Measuring Mathematical Problem Solving With the MATH Dataset},
  author={Dan Hendrycks and Collin Burns and Saurav Kadavath and Akul Arora and Steven Basart and Eric Tang and Dawn Song and Jacob Steinhardt},
  journal={NeurIPS},
  year={2021}
}

@article{bench_WiC,
  title={WiC: the Word-in-Context Dataset for Evaluating Context-Sensitive Meaning Representations},
  author={ Mohammad Taher Pilehvar and Jose Camacho-Collados},
  journal={Proceedings of NAACL 2019 (short)},
  year={2019}
}  

@article{HumanEval,
  title={Evaluating Large Language Models Trained on Code},
  author={Mark Chen and Jerry Tworek and Heewoo Jun and Qiming Yuan and Henrique Ponde de Oliveira Pinto and Jared Kaplan and Harri Edwards and Yuri Burda and Nicholas Joseph and Greg Brockman and Alex Ray and Raul Puri and Gretchen Krueger and Michael Petrov and Heidy Khlaaf and Girish Sastry and Pamela Mishkin and Brooke Chan and Scott Gray and Nick Ryder and Mikhail Pavlov and Alethea Power and Lukasz Kaiser and Mohammad Bavarian and Clemens Winter and Philippe Tillet and Felipe Petroski Such and Dave Cummings and Matthias Plappert and Fotios Chantzis and Elizabeth Barnes and Ariel Herbert-Voss and William Hebgen Guss and Alex Nichol and Alex Paino and Nikolas Tezak and Jie Tang and Igor Babuschkin and Suchir Balaji and Shantanu Jain and William Saunders and Christopher Hesse and Andrew N. Carr and Jan Leike and Josh Achiam and Vedant Misra and Evan Morikawa and Alec Radford and Matthew Knight and Miles Brundage and Mira Murati and Katie Mayer and Peter Welinder and Bob McGrew and Dario Amodei and Sam McCandlish and Ilya Sutskever and Wojciech Zaremba},
  year={2021},
  eprint={2107.03374},
  archivePrefix={arXiv},
  primaryClass={cs.LG}
}

@article{UncertaintyBench_24_arXiv_Tencent,
  title={Benchmarking llms via uncertainty quantification},
  author={Ye, Fanghua and Yang, Mingming and Pang, Jianhui and Wang, Longyue and Wong, Derek F and Yilmaz, Emine and Shi, Shuming and Tu, Zhaopeng},
  journal={arXiv preprint arXiv:2401.12794},
  year={2024}
}

@article{UBench_24_arXiv_Nankai,
  title={UBENCH: Benchmarking Uncertainty in Large Language Models with Multiple Choice Questions},
  author={Wang, Xunzhi and Zhang, Zhuowei and Li, Qiongyu and Chen, Gaonan and Hu, Mengting and Luo, Bitong and Gao, Hang and Han, Zhixin and Wang, Haotian and others},
  journal={arXiv preprint arXiv:2406.12784},
  year={2024}
}

@inproceedings{PopQA_23_GEM_IBM,
    title = "Predicting Question-Answering Performance of Large Language Models through Semantic Consistency",
    author = "Rabinovich, Ella  and
      Ackerman, Samuel  and
      Raz, Orna  and
      Farchi, Eitan  and
      Anaby Tavor, Ateret",
    editor = "Gehrmann, Sebastian  and
      Wang, Alex  and
      Sedoc, Jo{\~a}o  and
      Clark, Elizabeth  and
      Dhole, Kaustubh  and
      Chandu, Khyathi Raghavi  and
      Santus, Enrico  and
      Sedghamiz, Hooman",
    booktitle = "Proceedings of the Third Workshop on Natural Language Generation, Evaluation, and Metrics (GEM)",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.gem-1.12",
    pages = "138--154",
}

@inproceedings{CrossLingualConsistency_23_EMNLP_UoGroningen,
title={Cross-Lingual Consistency of Factual Knowledge in Multilingual Language Models},
author={Jirui Qi and Raquel Fern{\'a}ndez and Arianna Bisazza},
booktitle={The 2023 Conference on Empirical Methods in Natural Language Processing},
year={2023},
url={https://openreview.net/forum?id=MLKLYoXypN}
}

@inproceedings{BECEL_22_Coling_Oxford,
    title = "{BECEL}: Benchmark for Consistency Evaluation of Language Models",
    author = "Jang, Myeongjun  and
      Kwon, Deuk Sin  and
      Lukasiewicz, Thomas",
    editor = "Calzolari, Nicoletta  and
      Huang, Chu-Ren  and
      Kim, Hansaem  and
      Pustejovsky, James  and
      Wanner, Leo  and
      Choi, Key-Sun  and
      Ryu, Pum-Mo  and
      Chen, Hsin-Hsi  and
      Donatelli, Lucia  and
      Ji, Heng  and
      Kurohashi, Sadao  and
      Paggio, Patrizia  and
      Xue, Nianwen  and
      Kim, Seokhwan  and
      Hahm, Younggyun  and
      He, Zhong  and
      Lee, Tony Kyungil  and
      Santus, Enrico  and
      Bond, Francis  and
      Na, Seung-Hoon",
    booktitle = "Proceedings of the 29th International Conference on Computational Linguistics",
    month = oct,
    year = "2022",
    address = "Gyeongju, Republic of Korea",
    publisher = "International Committee on Computational Linguistics",
    url = "https://aclanthology.org/2022.coling-1.324",
    pages = "3680--3696",
}

@article{ParaRel_21_TACL_BarIlan,
    title = "Measuring and Improving Consistency in Pretrained Language Models",
    author = {Elazar, Yanai  and
      Kassner, Nora  and
      Ravfogel, Shauli  and
      Ravichander, Abhilasha  and
      Hovy, Eduard  and
      Sch{\"u}tze, Hinrich  and
      Goldberg, Yoav},
    editor = "Roark, Brian  and
      Nenkova, Ani",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "9",
    year = "2021",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/2021.tacl-1.60",
    doi = "10.1162/tacl_a_00410",
    pages = "1012--1031",
}

@article{CriticBench_24_arXiv_THU,
  title={CriticBench: Benchmarking LLMs for Critique-Correct Reasoning},
  author={Lin, Zicheng and Gou, Zhibin and Liang, Tian and Luo, Ruilin and Liu, Haowei and Yang, Yujiu},
  journal={arXiv preprint arXiv:2402.14809},
  year={2024}
}

@article{EvalSelf_24_arXiv_THU,
  title={Can I understand what I create? Self-Knowledge Evaluation of Large Language Models},
  author={Tan, Zhiquan and Wei, Lai and Wang, Jindong and Xie, Xing and Huang, Weiran},
  journal={arXiv preprint arXiv:2406.06140},
  year={2024}
}

@article{FTEvsTOE_24_arxiv_LMU,
  title={" My Answer is C": First-Token Probabilities Do Not Match Text Answers in Instruction-Tuned Language Models},
  author={Wang, Xinpeng and Ma, Bolei and Hu, Chengzhi and Weber-Genzel, Leon and R{\"o}ttger, Paul and Kreuter, Frauke and Hovy, Dirk and Plank, Barbara},
  journal={arXiv preprint arXiv:2402.14499},
  year={2024}
}

@article{wang2024answersreviewingrationalitymultiple,
  title={Beyond the answers: Reviewing the rationality of multiple choice question answering for the evaluation of large language models},
  author={Wang, Haochun and Zhao, Sendong and Qiang, Zewen and Qin, Bing and Liu, Ting},
  journal={arXiv preprint arXiv:2402.01349},
  year={2024}
}